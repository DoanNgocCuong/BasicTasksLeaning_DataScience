{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smLYnPIUxgbj",
        "outputId": "92113df8-b1c2-4fad-99a8-e1e298e5574e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S9gJowZi5ufJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9YIqI355ufL"
      },
      "source": [
        "Let’s create a DataFrame with which we can work:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "JIXSZ2MfyQTM",
        "outputId": "f0db649b-1ce4-4f77-9eb8-aa61eded1881"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/1. Courses/Big data - F/materials/spark-lab/data/flight-data/json/2015-summary.json\""
      ],
      "metadata": {
        "id": "DKlY5M1Ph7fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7twMPku5ufM"
      },
      "outputs": [],
      "source": [
        "# /content/drive/MyDrive/2. Persional Files/2022\n",
        "# path /content/drive/MyDrive/1. Courses/Big data - F/materials/spark-lab/data\n",
        "# /content/drive/MyDrive/1. Courses/Big data - F/materials/spark-lab/data\n",
        "df = spark.read.format(\"json\").load(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQgcuApB5ufN"
      },
      "source": [
        "We discussed that a DataFame will have columns, and we use a schema to define them. Let’s\n",
        "take a look at the schema on our current DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8y67cF35ufN"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPBDCijP5ufP"
      },
      "outputs": [],
      "source": [
        "df.schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvFtvmHX5ufP"
      },
      "source": [
        "A schema is a StructType made up of a number of fields, StructFields, that have a name,\n",
        "type, a Boolean flag which specifies whether that column can contain missing or null values,\n",
        "and, finally, users can optionally specify associated metadata with that column. The metadata is a\n",
        "way of storing information about this column (Spark uses this in its machine learning library)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W78HoQs65ufQ"
      },
      "source": [
        "The example that follows shows how to create and\n",
        "enforce a specific schema on a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrXpy7Bg5ufQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "myManualSchema = StructType([\n",
        "StructField(\"DEST_COUNTRY_NAME_XYZ\", StringType(), False),\n",
        "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
        "])\n",
        "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
        ".load(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnyo4KY45ufR"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8o1nK2I5ufR"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah61TE7W5ufS"
      },
      "source": [
        "# Columns and Expressions\n",
        "## Columns\n",
        "There are a lot of different ways to construct and refer to columns but the two simplest ways are\n",
        "by using the col or column functions. To use either of these functions, you pass in a column\n",
        "name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOMWwPUS5ufS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, column\n",
        "col(\"someColumnName\")\n",
        "column(\"someColumnName\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckz_gLYV5ufT"
      },
      "source": [
        "Explicit column references"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MowsprgI5ufT"
      },
      "outputs": [],
      "source": [
        "df[\"count\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVT0NmFr5ufT"
      },
      "outputs": [],
      "source": [
        "df.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eykqkj95ufU"
      },
      "outputs": [],
      "source": [
        "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSbkUMfq5ufU"
      },
      "source": [
        "# Records and Rows\n",
        "In Spark, each row in a DataFrame is a single record."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuAROMMY5ufU"
      },
      "outputs": [],
      "source": [
        "df.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SihReqqW5ufV"
      },
      "source": [
        "## Creating Rows\n",
        "You can create rows by manually instantiating a Row object with the values that belong in each\n",
        "column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNM__Is_5ufV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "myRow = Row(\"Hello\", None, 1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIZEpIus5ufV"
      },
      "outputs": [],
      "source": [
        "myRow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIQ4ngsg5ufV"
      },
      "source": [
        "Accessing data in rows is equally as easy: you just specify the position that you would like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-PdC58d5ufW"
      },
      "outputs": [],
      "source": [
        "print(myRow[0])\n",
        "print(myRow[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb4qbFya5ufW"
      },
      "source": [
        "# DataFrame Transformations\n",
        "1. We can add rows or columns\n",
        "2. We can remove rows or columns\n",
        "3. We can transform a row into a column (or vice versa)\n",
        "4. We can change the order of rows based on the values in columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK0jZEXV5ufW"
      },
      "source": [
        " ![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEmR5knF5ufW"
      },
      "source": [
        "## Creating DataFrames\n",
        "We can create DataFrames from raw data sources. We will also register this as a temporary view so that we\n",
        "can query it with SQL and show off basic transformations in SQL, as well)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ninew3p5ufW"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"json\").load(data_path)\n",
        "df.createOrReplaceTempView(\"dfTable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJxyjo4d5ufX"
      },
      "source": [
        "We can also create DataFrames on the fly by taking a set of rows and converting them to a\n",
        "DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzYggA0S5ufX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "myManualSchema = StructType([\n",
        "StructField(\"some\", StringType(), True),\n",
        "StructField(\"col\", StringType(), True),\n",
        "StructField(\"names\", LongType(), False)\n",
        "])\n",
        "myRow = Row(\"Hello\", None, 1)\n",
        "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
        "myDf.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "XwZ-IxiqiHug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKCJTqej5ufX"
      },
      "source": [
        "## select and selectExpr\n",
        "select and selectExpr allow you to do the DataFrame equivalent of SQL queries on a table of\n",
        "data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xXRYnvX5ufY"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
        "#-- in SQL\n",
        "#SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2\n",
        "spark.sql('SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2').show(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2')"
      ],
      "metadata": {
        "id": "xdH_0VWtiYi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"DEST_COUNTRY_NAME\"]"
      ],
      "metadata": {
        "id": "jJSEreewV4fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTlsdjbM5ufY"
      },
      "source": [
        "You can select multiple columns by using the same style of query, just add more column name\n",
        "strings to your select method call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU83kPZo5ufY"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
        "#-- in SQL\n",
        "#SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q54Kb50Y5ufY"
      },
      "source": [
        "As discussed in “Columns and Expressions”, you can refer to columns in a number of different\n",
        "ways; all you need to keep in mind is that you can use them interchangeably:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q90Rlamp5ufZ"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "from pyspark.sql.functions import expr, col, column\n",
        "df.select(\n",
        "expr(\"DEST_COUNTRY_NAME\"),\n",
        "col(\"DEST_COUNTRY_NAME\"),\n",
        "column(\"DEST_COUNTRY_NAME\"))\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPOY_Mex5ufZ"
      },
      "source": [
        "expr is the most flexible reference that we can use. It can refer to a plain\n",
        "column or a string manipulation of a column. To illustrate, let’s change the column name, and\n",
        "then change it back by using the AS keyword and then the alias method on the column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q47KuqdE5ufZ"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
        "#-- in SQL\n",
        "#SELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM2om04d5ufa"
      },
      "source": [
        "The preceding operation changes the column name back to its original name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLAmF1nm5ufa"
      },
      "outputs": [],
      "source": [
        "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9nzYUSr5ufa"
      },
      "source": [
        "Because select followed by a series of expr is such a common pattern, Spark has a shorthand\n",
        "for doing this efficiently: selectExpr. This is probably the most convenient interface for\n",
        "everyday use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__Uxd0P25ufb"
      },
      "outputs": [],
      "source": [
        "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T8pFOI15ufb"
      },
      "source": [
        "This opens up the true power of Spark. We can treat selectExpr as a simple way to build up\n",
        "complex expressions that create new DataFrames. In fact, we can add any valid non-aggregating\n",
        "SQL statement, and as long as the columns resolve, it will be valid! Here’s a simple example that\n",
        "adds a new column withinCountry to our DataFrame that specifies whether the destination and\n",
        "origin are the same:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CdTOKsa5ufb"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.selectExpr(\n",
        "\"*\", # all original columns\n",
        "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
        ".show(20)\n",
        "#-- in SQL\n",
        "#SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\n",
        "#FROM dfTable\n",
        "#LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhinT9M5ufb"
      },
      "source": [
        "With select expression, we can also specify aggregations over the entire DataFrame by taking\n",
        "advantage of the functions that we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrVMxOLC5ufb"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
        "#-- in SQL\n",
        "#SELECT avg(count), count(distinct(DEST_COUNTRY_NAME)) FROM dfTable LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1u0Iv655ufc"
      },
      "source": [
        "## Converting to Spark Types (Literals)\n",
        "Sometimes, we need to pass explicit values into Spark that are just a value (rather than a new\n",
        "column). This might be a constant value or something we’ll need to compare to later on. The\n",
        "way we do this is through literals. This is basically a translation from a given programming\n",
        "language’s literal value to one that Spark understands. Literals are expressions and you can use\n",
        "them in the same way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTtjrOsy5ufc"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "from pyspark.sql.functions import lit\n",
        "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
        "\n",
        "#In SQL, literals are just the specific value:\n",
        "#-- in SQL\n",
        "#SELECT *, 1 as One FROM dfTable LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztoMqCVH5ufc"
      },
      "source": [
        "## Adding Columns\n",
        "There’s also a more formal way of adding a new column to a DataFrame, and that’s by using the\n",
        "withColumn method on our DataFrame. For example, let’s add a column that just adds the\n",
        "number one as a column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfncuZfW5ufc"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
        "#-- in SQL\n",
        "#SELECT *, 1 as numberOne FROM dfTable LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laE6-F5H5ufd"
      },
      "source": [
        "Let’s do something a bit more interesting and make it an actual expression. In the next example,\n",
        "we’ll set a Boolean flag for when the origin country is the same as the destination country:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjReGwz25ufd"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxd2EXov5ufd"
      },
      "source": [
        "Notice that the withColumn function takes two arguments: the column name and the expression\n",
        "that will create the value for that given row in the DataFrame. Interestingly, we can also rename\n",
        "a column this way. The SQL syntax is the same as we had previously, so we can omit it in this\n",
        "example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1uUc_2E5ufd"
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP5-m_fWxGV0"
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuiN7rAB5ufd"
      },
      "source": [
        "## Renaming Columns\n",
        "Although we can rename a column in the manner that we just described, another alternative is to\n",
        "use the withColumnRenamed method. This will rename the column with the name of the string in\n",
        "the first argument to the string in the second argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OuWxktk5ufe"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPeezS0B5ufe"
      },
      "source": [
        "## Reserved Characters and Keywords\n",
        "One thing that you might come across is reserved characters like spaces or dashes in column\n",
        "names. Handling these means escaping column names appropriately. In Spark, we do this by\n",
        "using backtick (`) characters. Let’s use withColumn, which you just learned about to create a\n",
        "column with reserved characters. We’ll show two examples—in the one shown here, we don’t\n",
        "need escape characters, but in the next one, we do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SFQ15Xb5ufe"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "dfWithLongColName = df.withColumn(\n",
        "\"This Long Column-Name\",\n",
        "expr(\"ORIGIN_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pa3HYrR5ufe"
      },
      "source": [
        "We don’t need escape characters here because the first argument to withColumn is just a string\n",
        "for the new column name. In this example, however, we need to use backticks because we’re\n",
        "referencing a column in an expression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grXSiTUD5ufe"
      },
      "outputs": [],
      "source": [
        "dfWithLongColName.selectExpr(\n",
        "\"`This Long Column-Name`\",\n",
        "\"`This Long Column-Name` as `new col`\")\\\n",
        ".show(2)\n",
        "dfWithLongColName.createOrReplaceTempView(\"dfTableLong\")\n",
        "#-- in SQL\n",
        "#SELECT `This Long Column-Name`, `This Long Column-Name` as `new col`\n",
        "#FROM dfTableLong LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjJ3fXjm5uff"
      },
      "source": [
        "We can refer to columns with reserved characters (and not escape them) if we’re doing an\n",
        "explicit string-to-column reference, which is interpreted as a literal instead of an expression. We\n",
        "only need to escape expressions that use reserved characters or keywords. The following two\n",
        "examples both result in the same DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ll1_FEY5uff"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KsxHwDp5uff"
      },
      "source": [
        "## Removing Columns\n",
        "Now that we’ve created this column, let’s take a look at how we can remove columns from\n",
        "DataFrames. You likely already noticed that we can do this by using select. However, there is\n",
        "also a dedicated method called drop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPk1hyX35ufg"
      },
      "outputs": [],
      "source": [
        "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM71hJp-5ufg"
      },
      "source": [
        "We can drop multiple columns by passing in multiple columns as arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPwK6kKN5ufg"
      },
      "outputs": [],
      "source": [
        "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3e_IxmP5ufg"
      },
      "source": [
        "## Changing a Column’s Type (cast)\n",
        "Sometimes, we might need to convert from one type to another; for example, if we have a set of\n",
        "StringType that should be integers. We can convert columns from one type to another by\n",
        "casting the column from one type to another. For instance, let’s convert our count column from\n",
        "an integer to a type Long:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twzo0vQP5ufg"
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"count2\", col(\"count\").cast(\"long\"))\n",
        "#-- in SQL\n",
        "#SELECT *, cast(count as long) AS count2 FROM dfTable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT3706_i5ufh"
      },
      "source": [
        "## Filtering Rows\n",
        "To filter rows, we create an expression that evaluates to true or false. You then filter out the rows\n",
        "with an expression that is equal to false. The most common way to do this with DataFrames is to\n",
        "create either an expression as a String or build an expression by using a set of column\n",
        "manipulations. There are two methods to perform this operation: you can use where or filter\n",
        "and they both will perform the same operation and accept the same argument types when used\n",
        "with DataFrames. We will stick to where because of its familiarity to SQL; however, filter is\n",
        "valid as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tok106pW5ufh"
      },
      "outputs": [],
      "source": [
        "#The following filters are equivalent, and the results are the same in Scala and Python:\n",
        "df.filter(col(\"count\") < 2).show(2)\n",
        "df.where(\"count < 2\").show(2)\n",
        "#-- in SQL\n",
        "#SELECT * FROM dfTable WHERE count < 2 LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg6wVy4F5ufh"
      },
      "source": [
        "Instinctually, you might want to put multiple filters into the same expression. Although this is\n",
        "possible, it is not always useful, because Spark automatically performs all filtering operations at\n",
        "the same time regardless of the filter ordering. This means that if you want to specify multiple\n",
        "AND filters, just chain them sequentially and let Spark handle the rest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTtCYwJt5ufh"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n",
        ".show(2)\n",
        "#-- in SQL\n",
        "#SELECT * FROM dfTable WHERE count < 2 AND ORIGIN_COUNTRY_NAME != \"Croatia\"\n",
        "#LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tivn-IPX5ufh"
      },
      "source": [
        "## Getting Unique Rows\n",
        "A very common use case is to extract the unique or distinct values in a DataFrame. These values\n",
        "can be in one or more columns. The way we do this is by using the distinct method on a\n",
        "DataFrame, which allows us to deduplicate any rows that are in that DataFrame. For instance,\n",
        "let’s get the unique origins in our dataset. This, of course, is a transformation that will return a\n",
        "new DataFrame with only unique rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHsEHISS5ufi"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n",
        "#-- in SQL\n",
        "#SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8M-b4fn5ufi"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n",
        "#-- in SQL\n",
        "#SELECT COUNT(DISTINCT ORIGIN_COUNTRY_NAME) FROM dfTable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnaTgV9J5ufi"
      },
      "source": [
        "## Random Samples\n",
        "Sometimes, you might just want to sample some random records from your DataFrame. You can\n",
        "do this by using the sample method on a DataFrame, which makes it possible for you to specify\n",
        "a fraction of rows to extract from a DataFrame and whether you’d like to sample with or without\n",
        "replacement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vBkO1N35ufi"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "seed = 5\n",
        "withReplacement = False\n",
        "fraction = 0.5\n",
        "df.sample(withReplacement, fraction, seed).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nR8_0wS5ufj"
      },
      "source": [
        "## Random Splits\n",
        "Random splits can be helpful when you need to break up your DataFrame into a random “splits”\n",
        "of the original DataFrame. This is often used with machine learning algorithms to create training,\n",
        "validation, and test sets. In this next example, we’ll split our DataFrame into two different\n",
        "DataFrames by setting the weights by which we will split the DataFrame (these are the\n",
        "arguments to the function). Because this method is designed to be randomized, we will also\n",
        "specify a seed (just replace seed with a number of your choosing in the code block). It’s\n",
        "important to note that if you don’t specify a proportion for each DataFrame that adds up to one,\n",
        "they will be normalized so that they do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIUcuYOV5ufj"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
        "dataFrames[0].count() > dataFrames[1].count() # False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4V02G5p5ufj"
      },
      "source": [
        "## Concatenating and Appending Rows (Union)\n",
        "As you learned in the previous section, DataFrames are immutable. This means users cannot\n",
        "append to DataFrames because that would be changing it. To append to a DataFrame, you must\n",
        "union the original DataFrame along with the new DataFrame. This just concatenates the two\n",
        "DataFramess. To union two DataFrames, you must be sure that they have the same schema and\n",
        "number of columns; otherwise, the union will fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0983rsl5ufj"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "from pyspark.sql import Row\n",
        "schema = df.schema\n",
        "newRows = [\n",
        "Row(\"New Country\", \"Other Country\", 5),\n",
        "Row(\"New Country 2\", \"Other Country 3\", 1)\n",
        "]\n",
        "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
        "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
        "# in Python\n",
        "df.union(newDF)\\\n",
        ".where(\"count = 1\")\\\n",
        ".where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
        ".show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFoMVOJo5ufk"
      },
      "source": [
        "## Sorting Rows\n",
        "When we sort the values in a DataFrame, we always want to sort with either the largest or\n",
        "smallest values at the top of a DataFrame. There are two equivalent operations to do this sort\n",
        "and orderBy that work the exact same way. They accept both column expressions and strings as\n",
        "well as multiple columns. The default is to sort in ascending order:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cegXe5uN5ufk"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.sort(\"count\").show(5)\n",
        "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
        "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9u6eg3r5ufk"
      },
      "source": [
        "To more explicitly specify sort direction, you need to use the asc and desc functions if operating\n",
        "on a column. These allow you to specify the order in which a given column should be sorted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8BGaY7f5ufk"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "from pyspark.sql.functions import desc, asc\n",
        "df.orderBy(expr(\"count desc\")).show(2)\n",
        "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n",
        "#-- in SQL\n",
        "#SELECT * FROM dfTable ORDER BY count DESC, DEST_COUNTRY_NAME ASC LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euLMA_BW5ufl"
      },
      "source": [
        "An advanced tip is to use asc_nulls_first, desc_nulls_first, asc_nulls_last, or\n",
        "desc_nulls_last to specify where you would like your null values to appear in an ordered\n",
        "DataFrame.\n",
        "For optimization purposes, it’s sometimes advisable to sort within each partition before another\n",
        "set of transformations. You can use the sortWithinPartitions method to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTyknw7L5ufl"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "#spark.read.format(\"json\").load(\"../data/flight-data/json/*-summary.json\")\\\n",
        "spark.read.format(\"json\").load(data_path)\\\n",
        ".sortWithinPartitions(\"count\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8TJ00en5ufl"
      },
      "source": [
        "## Limit\n",
        "Oftentimes, you might want to restrict what you extract from a DataFrame; for example, you\n",
        "might want just the top ten of some DataFrame. You can do this by using the limit method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhOXMZCa5ufl"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.limit(5).show()\n",
        "#-- in SQL\n",
        "#SELECT * FROM dfTable LIMIT 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sDDgjYy5ufm"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.orderBy(expr(\"count desc\")).limit(6).show()\n",
        "#-- in SQL\n",
        "#SELECT * FROM dfTable ORDER BY count desc LIMIT 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opf5HaAg5ufm"
      },
      "source": [
        "## Repartition and Coalesce\n",
        "Another important optimization opportunity is to partition the data according to some frequently\n",
        "filtered columns, which control the physical layout of data across the cluster including the\n",
        "partitioning scheme and the number of partitions.\n",
        "\n",
        "Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This\n",
        "means that you should typically only repartition when the future number of partitions is greater\n",
        "than your current number of partitions or when you are looking to partition by a set of columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leesODaC5ufm"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.rdd.getNumPartitions() # 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtaTMQ6GxGV2"
      },
      "outputs": [],
      "source": [
        "df.repartition(5).repartition(3).coalesce(2).rdd.getNumPartitions() # 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0CkVIzl5ufm"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.repartition(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1ulOsxq5ufn"
      },
      "source": [
        "If you know that you’re going to be filtering by a certain column often, it can be worth\n",
        "repartitioning based on that column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rKWeUYa5ufn"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhmeR6R5ufn"
      },
      "source": [
        "You can optionally specify the number of partitions you would like, too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81Sk32WW5ufn"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysyNq7QE5ufn"
      },
      "source": [
        "Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. This\n",
        "operation will shuffle your data into five partitions based on the destination country name, and\n",
        "then coalesce them (without a full shuffle):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrIoqjIB5ufn"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6i1gGAT5ufo"
      },
      "source": [
        "## Collecting Rows to the Driver\n",
        "As discussed in previous chapters, Spark maintains the state of the cluster in the driver. There are\n",
        "times when you’ll want to collect some of your data to the driver in order to manipulate it on\n",
        "your local machine.\n",
        "\n",
        "Thus far, we did not explicitly define this operation. However, we used several different methods\n",
        "for doing so that are effectively all the same. collect gets all data from the entire DataFrame,\n",
        "take selects the first N rows, and show prints out a number of rows nicely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-64OG9Kw5ufo"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "collectDF = df.limit(10)\n",
        "collectDF.take(5) # take works with an Integer count\n",
        "collectDF.show() # this prints it out nicely\n",
        "collectDF.show(5, False)\n",
        "collectDF.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ZqgbrE5ufo"
      },
      "source": [
        "There’s an additional way of collecting rows to the driver in order to iterate over the entire\n",
        "dataset. The method toLocalIterator collects partitions to the driver as an iterator. This\n",
        "method allows you to iterate over the entire dataset partition-by-partition in a serial manner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA2bmHdo5ufo"
      },
      "outputs": [],
      "source": [
        "collectDF.toLocalIterator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5o_Co465ufp"
      },
      "source": [
        "# Conclusion\n",
        "This chapter covered basic operations on DataFrames. You learned the simple concepts and tools\n",
        "that you will need to be successful with Spark DataFrames. Chapter 6 covers in much greater\n",
        "detail all of the different ways in which you can manipulate the data in those DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJwye9Xu5ufp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SihReqqW5ufV",
        "eEmR5knF5ufW",
        "UKCJTqej5ufX",
        "x1u0Iv655ufc",
        "ztoMqCVH5ufc",
        "PuiN7rAB5ufd",
        "zPeezS0B5ufe",
        "6KsxHwDp5uff",
        "g3e_IxmP5ufg",
        "xT3706_i5ufh",
        "Tivn-IPX5ufh",
        "mnaTgV9J5ufi",
        "6nR8_0wS5ufj",
        "C4V02G5p5ufj",
        "nFoMVOJo5ufk",
        "F8TJ00en5ufl",
        "Opf5HaAg5ufm",
        "N6i1gGAT5ufo"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}